{
    "activation":["relu", "tanh"],
    "hidden_layer_sizes": [[50], [100], [50,50], [50,100,50]],
    "solver": ["lbfgs", "sgd", "adam"],
    "alpha": [1e-5, 0.1, "uniform"],
    "learning_rate": ["constant", "invscaling", "adaptive"],
    "learning_rate_init": [1e-5, 0.1, "uniform"]
}